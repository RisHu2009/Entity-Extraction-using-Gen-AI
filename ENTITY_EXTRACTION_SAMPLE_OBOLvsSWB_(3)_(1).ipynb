{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RisHu2009/Test/blob/master/ENTITY_EXTRACTION_SAMPLE_OBOLvsSWB_(3)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing google-generativeai library to work with any google models\n",
        "!pip install -q -U google-generativeai\n",
        "\n",
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n"
      ],
      "metadata": {
        "id": "IR-Z7V7NW0Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4644a0ea-d3a5-4c67-a2f8-1bcc417cb3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/164.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m163.8/164.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/718.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J9alTR_LPKv",
        "outputId": "aa1c2ce5-1dbe-4975-852f-915955c3aa3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -U langchain-community\n",
        "!pip install langchain\n",
        "!pip install PyPDF2  # Corrected package name\n",
        "# import google.ai.generativeai as genai\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "!pip install pypdf\n",
        "\n",
        "\n",
        "# Load content from the first PDF\n",
        "loader1 = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/OBOL_SAMPLE.pdf\")\n",
        "pages1 = loader1.load()\n",
        "\n",
        "# Load content from the second PDF\n",
        "loader2 = PyPDFLoader(\"/content/drive/MyDrive/Colab Notebooks/SWB_SAMPLE.pdf\")\n",
        "pages2 = loader2.load()\n",
        "\n",
        "# Replace with your actual Gemini API key\n",
        "gemini_api_key = \"AIzaSyBrwQjMeEO2RprYmTVU8pYCqTp9q0m8lkY\"\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "# Set up the model\n",
        "generation_config = {\n",
        "  \"temperature\": 0.9,\n",
        "  \"top_p\": 1,\n",
        "  \"top_k\": 1,\n",
        "  \"max_output_tokens\": 2048,\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "]\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.0-pro\",\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)\n",
        "\n",
        "convo = model.start_chat(history=[])\n",
        "\n",
        "# Store extracted entities from both PDFs\n",
        "all_entities = {'pdf1': {}, 'pdf2': {}}\n",
        "\n",
        "# Extract entities from the first PDF\n",
        "for page in pages1:\n",
        "    page_content = page.page_content\n",
        "\n",
        "    for block in page_content.split(\"\\n\\n\"):\n",
        "        response = convo.send_message(f\"Extract entities (Booking No, Address, Order No, Vessel name, OBOL number, Depart date, Cutoff date, Destination port, etc.) from this text:\\n{block}\")\n",
        "        extracted_entities = response.text.split(\"\\n\")\n",
        "        for entity in extracted_entities:\n",
        "            try:\n",
        "                entity_type, entity_value = entity.split(\":\", 1)\n",
        "                all_entities['pdf1'][entity_type.strip()] = entity_value.strip()\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "# Extract entities from the second PDF\n",
        "for page in pages2:\n",
        "    page_content = page.page_content\n",
        "\n",
        "    for block in page_content.split(\"\\n\\n\"):\n",
        "        response = convo.send_message(f\"Extract entities (Sea waybill no, Voyage no, Booking Ref, Port of Discharge, Date of issue, Onboard date, SCAC code etc.) from this text:\\n{block}\")\n",
        "        extracted_entities = response.text.split(\"\\n\")\n",
        "        for entity in extracted_entities:\n",
        "            try:\n",
        "                entity_type, entity_value = entity.split(\":\", 1)\n",
        "                all_entities['pdf2'][entity_type.strip()] = entity_value.strip()\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "# Print extracted entities with headings\n",
        "for pdf, entities in all_entities.items():\n",
        "    print(f\"Extracted entities from {pdf}:\")\n",
        "    for key, value in entities.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3MberP57XcPG",
        "outputId": "808ff891-aa43-4928-c3f2-c5acd8c92eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.8)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.19)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.19 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.86)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.19->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.19->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.19->langchain) (3.0.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Extracted entities from pdf1:\n",
            "**Booking No: ** 038VC9102909\n",
            "**Address: ** GRAFENAUWEG 4, 6300 ZUG, SWITZERLAND\n",
            "**Order No: ** 0000027759-10\n",
            "**Vessel name: ** MSC HEIDI\n",
            "**OBOL number: ** MEDUUH637842\n",
            "**Depart date: ** 23-Jul-2023\n",
            "**Cutoff date: ** 20-Jul-2023\n",
            "**Destination port: ** NLRTM\n",
            "**Total ROLL Shipped: ** 36\n",
            "**Total Lots Shipped: ** 18\n",
            "**Total Containers: ** 1\n",
            "**CBM: ** 32.292\n",
            "**HTS Code: ** 4703.21\n",
            "\n",
            "Extracted entities from pdf2:\n",
            "**Sea waybill no: ** MEDUUH637842\n",
            "**Voyage no: ** NG328E\n",
            "**Booking Ref: ** Not provided in the given text.\n",
            "**Port of Discharge: ** Not provided in the given text.\n",
            "**Date of issue: ** 23-JULY-2023\n",
            "**Onboard date: ** 23-JULY-2023\n",
            "**SCAC code: ** MSCU\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Clean up extracted entities\n",
        "# cleaned_entities = {}\n",
        "# for key, value in entities.items():\n",
        "#     cleaned_key = key.replace('**', '').strip()\n",
        "#     cleaned_value = value.replace('**', '').strip()\n",
        "#     cleaned_entities[cleaned_key] = cleaned_value\n",
        "\n",
        "# # Print cleaned extracted entities on separate lines\n",
        "# for key, value in cleaned_entities.items():\n",
        "#     print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "k79DDO-MRl5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing extracted values in excel sheets\n",
        "import pandas as pd\n",
        "\n",
        "# Convert dictionaries to pandas DataFrame\n",
        "df_pdf1 = pd.DataFrame.from_dict(all_entities['pdf1'], orient='index', columns=['Entity Value'])\n",
        "df_pdf1.index.name = 'Entity Type'\n",
        "df_pdf2 = pd.DataFrame.from_dict(all_entities['pdf2'], orient='index', columns=['Entity Value'])\n",
        "df_pdf2.index.name = 'Entity Type'\n",
        "\n",
        "# Write DataFrames to separate Excel sheets\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/Colab Notebooks/extracted_entities.xlsx') as writer:\n",
        "    df_pdf1.to_excel(writer, sheet_name='Entities from PDF1')\n",
        "    df_pdf2.to_excel(writer, sheet_name='Entities from PDF2')\n",
        "\n",
        "print(\"Entities extracted from both PDFs are stored in 'extracted_entities.xlsx' file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6oVyCTSN3Gy",
        "outputId": "f4e3ad24-23ea-4878-a619-983f05c63938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities extracted from both PDFs are stored in 'extracted_entities.xlsx' file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparing entity values\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the extracted entities from both Excel sheets\n",
        "df1 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/extracted_entities.xlsx', sheet_name='Entities from PDF1')\n",
        "df2 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/extracted_entities.xlsx', sheet_name='Entities from PDF2')\n",
        "\n",
        "# Get the values from each DataFrame\n",
        "values_pdf1 = df1['Entity Value'].tolist()\n",
        "values_pdf2 = df2['Entity Value'].tolist()\n",
        "\n",
        "# Initialize lists to store matching and non-matching keys\n",
        "matched_keys = []\n",
        "non_matched_keys = []\n",
        "\n",
        "# Iterate over values from PDF1 and check for matches in PDF2\n",
        "for value_pdf1 in values_pdf1:\n",
        "    if value_pdf1 in values_pdf2:\n",
        "        matched_keys.append(value_pdf1)\n",
        "    else:\n",
        "        non_matched_keys.append(value_pdf1)\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "comparison_df = pd.DataFrame({'Values in pdf1': matched_keys + non_matched_keys,\n",
        "                              'Values in pdf2': [''] * len(matched_keys) + [''] * len(non_matched_keys),\n",
        "                              'Result': ['Keys are matched'] * len(matched_keys) + ['Keys are not matched'] * len(non_matched_keys)})\n",
        "\n",
        "# Save the comparison results to a new Excel file\n",
        "comparison_df.to_excel('/content/drive/MyDrive/Colab Notebooks/entity_comparison_result.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "xGDfO8ARtIrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing comparison results\n",
        "import pandas as pd\n",
        "\n",
        "# Load comparison results from Excel file\n",
        "def display_comparison_results():\n",
        "    comparison_df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/entity_comparison_result.xlsx')\n",
        "    print(\"Comparison results:\")\n",
        "    print(comparison_df)  # Print comparison results DataFrame\n",
        "\n",
        "# Call the function to display comparison results\n",
        "display_comparison_results()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar_QbKHASEH6",
        "outputId": "c60aca1f-031a-4d0f-ae9e-96392fc27202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison results:\n",
            "                             Values in pdf1  Values in pdf2  \\\n",
            "0                           ** MEDUUH637842             NaN   \n",
            "1                           ** 038VC9102909             NaN   \n",
            "2   ** GRAFENAUWEG 4, 6300 ZUG, SWITZERLAND             NaN   \n",
            "3                          ** 0000027759-10             NaN   \n",
            "4                              ** MSC HEIDI             NaN   \n",
            "5                            ** 23-Jul-2023             NaN   \n",
            "6                            ** 20-Jul-2023             NaN   \n",
            "7                                  ** NLRTM             NaN   \n",
            "8                                     ** 36             NaN   \n",
            "9                                     ** 18             NaN   \n",
            "10                                     ** 1             NaN   \n",
            "11                                ** 32.292             NaN   \n",
            "12                               ** 4703.21             NaN   \n",
            "\n",
            "                  Result  \n",
            "0       Keys are matched  \n",
            "1   Keys are not matched  \n",
            "2   Keys are not matched  \n",
            "3   Keys are not matched  \n",
            "4   Keys are not matched  \n",
            "5   Keys are not matched  \n",
            "6   Keys are not matched  \n",
            "7   Keys are not matched  \n",
            "8   Keys are not matched  \n",
            "9   Keys are not matched  \n",
            "10  Keys are not matched  \n",
            "11  Keys are not matched  \n",
            "12  Keys are not matched  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompting\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to display extracted entities based on user input\n",
        "def display_entities():\n",
        "    prompt = input(\"Enter your query or choose an option (pdf1, pdf2, Extraction Result, Comparison Result): \").lower()\n",
        "    if prompt == \"pdf1\":\n",
        "        print(\"Extracted entities from pdf1:\")\n",
        "        for key, value in all_entities['pdf1'].items():\n",
        "            print(f\"{key}: {value}\")\n",
        "    elif prompt == \"pdf2\":\n",
        "        print(\"Extracted entities from pdf2:\")\n",
        "        for key, value in all_entities['pdf2'].items():\n",
        "            print(f\"{key}: {value}\")\n",
        "    elif prompt == \"extraction result\":\n",
        "        print(\"Extracted entities from Extraction Result:\")\n",
        "        # Print extracted entities from both PDFs\n",
        "        for pdf, entities in all_entities.items():\n",
        "            print(f\"Extracted entities from {pdf}:\")\n",
        "            for key, value in entities.items():\n",
        "                print(f\"{key}: {value}\")\n",
        "    elif \"comparison\" in prompt:\n",
        "        try:\n",
        "            # Load the comparison results from the Excel file\n",
        "            comparison_df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/entity_comparison_result.xlsx')\n",
        "            print(\"Comparison results:\")\n",
        "            print(comparison_df)\n",
        "        except FileNotFoundError:\n",
        "            print(\"Comparison results file not found.\")\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'pdf1', 'pdf2', 'Extraction Result', or 'Comparison Result'.\")\n",
        "\n",
        "# Call the function to display extracted entities\n",
        "display_entities()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgTd6p6gT8X-",
        "outputId": "b207af8e-376d-43fa-b396-cea758879535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query or choose an option (pdf1, pdf2, Extraction Result, Comparison Result): pdf1\n",
            "Extracted entities from pdf1:\n",
            "**Booking No: ** 038VC9102909\n",
            "**Address: ** GRAFENAUWEG 4, 6300 ZUG, SWITZERLAND\n",
            "**Order No: ** 0000027759-10\n",
            "**Vessel name: ** MSC HEIDI\n",
            "**OBOL number: ** MEDUUH637842\n",
            "**Depart date: ** 23-Jul-2023\n",
            "**Cutoff date: ** 20-Jul-2023\n",
            "**Destination port: ** NLRTM\n",
            "**Total ROLL Shipped: ** 36\n",
            "**Total Lots Shipped: ** 18\n",
            "**Total Containers: ** 1\n",
            "**CBM: ** 32.292\n",
            "**HTS Code: ** 4703.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Load the extracted entities from both Excel sheets\n",
        "# df1 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/extracted_entities.xlsx', sheet_name='Entities from PDF1')\n",
        "# df2 = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/extracted_entities.xlsx', sheet_name='Entities from PDF2')\n",
        "\n",
        "# # Merge the DataFrames on the 'Entity Type' column\n",
        "# merged_df = pd.merge(df1, df2, how='outer', on=['Entity Type'], suffixes=('_pdf1', '_pdf2'))\n",
        "\n",
        "# # Create a new column to indicate whether the values are the same or not\n",
        "# merged_df['Comparison'] = 'Not same'\n",
        "# merged_df.loc[merged_df['Entity Value_pdf1'] == merged_df['Entity Value_pdf2'], 'Comparison'] = 'Same'\n",
        "\n",
        "# # Save the comparison results to a new Excel file\n",
        "# merged_df.to_excel('/content/drive/MyDrive/Colab Notebooks/entity_comparison.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "m5qQ0WzbS5My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Columns in df1:\", df1.columns)\n",
        "# print(\"Columns in df2:\", df2.columns)\n"
      ],
      "metadata": {
        "id": "41XdK-VJRT4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}